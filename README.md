# Sales_prediction_dataset_t4_codesoft
#predicting the sales data like Amazon 
# -*- coding: utf-8 -*-
"""Sales_data_t4_predict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QNuYBdFmSEYOJrl4XRVyx-W9h9756gik
"""

!pip install --upgrade scikit-learn

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from google.colab import files
import sklearn
import matplotlib.pyplot as plt

print(sklearn.__version__)

# Load dataset
uploaded = files.upload()
data = pd.read_csv('Sales_data.csv')

# Check for missing values and handle them
if data.isnull().sum().any():
    data.ffill(inplace=True)  # Forward fill for simplicity

# Convert date column to datetime if it exists
if 'date' in data.columns:
    data['date'] = pd.to_datetime(data['date'])
    # Extract features from date (example: year, month, day)
    data['year'] = data['date'].dt.year
    data['month'] = data['date'].dt.month
    data['day'] = data['date'].dt.day
    data = data.drop('date', axis=1)  # Drop the original date column

# Clean and convert 'discounted_price' to numeric
data['discounted_price'] = data['discounted_price'].astype(str).str.replace('â‚¹', '', regex=False).str.replace(',', '', regex=False)
data['discounted_price'] = pd.to_numeric(data['discounted_price'], errors='coerce')  # Handle conversion errors

# Separate target variable - Using 'discounted_price' as the target
y = data['discounted_price']
X = data.drop('discounted_price', axis=1)

# Identify categorical and numerical features
categorical_features = X.select_dtypes(include=['object', 'category']).columns
numerical_features = X.select_dtypes(include=np.number).columns

# Create a column transformer to apply different preprocessing steps to different columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', MinMaxScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ],
    remainder='passthrough'  # Keep other columns (if any)
)

# Create a pipeline that first preprocesses the data and then trains the model
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5))
])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)  # Split before preprocessing

# Train XGBoost model
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)

# Check the types and shapes of y_test and predictions
print(f'y_test type: {type(y_test)}, shape: {y_test.shape}')
print(f'predictions type: {type(predictions)}, shape: {predictions.shape}')

# Ensure both y_test and predictions are numeric
y_test = np.array(y_test)  # Convert to numpy array if not already
predictions = np.array(predictions)  # Convert to numpy array if not already

# Calculate RMSE manually
rmse = np.sqrt(np.mean((y_test - predictions) ** 2))
print(f'RMSE: {rmse:.2f}')

# Optional: Feature importance
# Access the regressor from the pipeline to get feature importance
xgb_model = model.named_steps['regressor']
xgb.plot_importance(xgb_model)
plt.title('Feature Importance')
plt.show()
